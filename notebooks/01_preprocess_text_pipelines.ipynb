{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.data.preprocess_text_helpers import (\n",
    "    contractions_unpacker,\n",
    "    punctuation_cleaner,\n",
    "    remove_stopwords,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "from src.data.preprocess_text_pipelines import (\n",
    "  clean,\n",
    "  tokenize,\n",
    "  normalize,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's start by reading in the data. The data consists of comments and their labels, 1 if it is misogynistic and 0 if not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @SwarajyaMag #GermanProfessor gives meaning to...\n",
       "1    #MKR Annie's never cooked on a BBQ before. See...\n",
       "2    RT @asredasmyhair: Feminists, take note. #FemF...\n",
       "3    @ChrisMMcDougall  He asked for it.  Did you?  ...\n",
       "4    RT @VILLEGOD I always really wish i had a girl...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/external/hatespeech/hatespeech_data_en.csv\")\n",
    "df['text'] = df['content']\n",
    "df.head()['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see if we take a look at social media text, is that it is incredibly messy: spelling mistakes, grammar failures and emojis galore. How are we going to tidy this up so we can begin to prepare this diamond in the rough dataset so we can understand better the sentiment of the tweets? We need to preprocess the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many steps you can take for this, and there is not one right answer, it is a case by case process. Nevertheless, Opt Out tries to make it easy for someone to get through the boring preprocessing work and into the nitty gritty text analyitics. Let's see what we can do in Opt Out\n",
    "\n",
    "\n",
    "We can either clean, tokenize or normalize the text. Let's start with clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tcake/coding_projects/python/opt_out/find-out/find-out/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    @swarajyamag #germanprofessor gives meaning to...\n",
       "1    #mkr annie ' s never cooked on a bbq before . ...\n",
       "2    rt @asredasmyhair : feminists , take note . #f...\n",
       "3    @chrismmcdougall he asked for it . did you ? w...\n",
       "4    rt @villegod i always really wish i had a girl...\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(df).head()['tokenized']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can you see that's different? Lowercase? Is the punctuation still there? Yes and yes. So what's actually involved in the cleaning process. Let's take an average trolly sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "troll_speak = \"RT @baum_erik: Lol I'm not surprised these 2 accounts blocked me @femfreq #FemiNazi#Gamergate &amp; @MomsAgainstWWE #ParanoidParent \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messy, gross, how are we going to understand more about this sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @baum_erik: Lol I am not surprised these 2 accounts blocked me @femfreq #FemiNazi#Gamergate &amp; @MomsAgainstWWE #ParanoidParent '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions_unpacker(troll_speak) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See I'm has gone to I am? that's called unpacking a contraction, and there are loads in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @baum_erik : Lol I ' m not surprised these 2 accounts blocked me @femfreq #FemiNazi #Gamergate & @MomsAgainstWWE #ParanoidParent\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(troll_speak)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok doesn't seem particulary interesting, but think about it. How would you normally split text up? by white space? But we'd lose the joined hashtags if that was the case. Our tokenizer handles that beautitfully. Now let's compare the results of tokenize to clean. Can you tell the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @swarajyamag #germanprofessor gives meaning te...\n",
       "1                #mkr annie never cooked bbq see alien\n",
       "2    rt @asredasmyhair feminists take note #femfree...\n",
       "3    @chrismmcdougall he asked did ? why find emplo...\n",
       "4    rt @villegod i always really wish girlfriend h...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(df).head()['cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference? Yup, punctuation and the removal of something call stopwords. Stopwords are unimportant words, like and, with. These words are important, but not for modeling. The extra steps we take to get here are show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @baum_erik: Lol I'm not surprised these 2 accounts blocked me @femfreq #FemiNazi#Gamergate &amp; @MomsAgainstWWE #ParanoidParent \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_cleaner(troll_speak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asfaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords(troll_speak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fafaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "You can't omit/backoff and unpack hashtags!\n",
      " unpack_hashtags will be set to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tcake/coding_projects/python/opt_out/find-out/find-out/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    <user> <hashtag> gives meaning term feminazi h...\n",
       "1           <hashtag> annie never cooked bbq see alien\n",
       "2    rt <user> feminists take note <hashtag> <hasht...\n",
       "3    <user> he asked did ? why find employer value ...\n",
       "4    rt <user> i always really wish girlfriend hung...\n",
       "Name: normalized, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(df).head()['normalized'] # suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this one is a little more involved, but it produces a little cooler results. I love this method. What is does it normalize the text, we don't really care about the different urls, hashtags etc in the text, we care about the number of them per tweet. This method allows us to not care, but care all at the same time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (find-out)",
   "language": "python",
   "name": "pycharm-66275036"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
